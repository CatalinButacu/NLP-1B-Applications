# Natural Language Processing: Complete Assignment Portfolio

## Overview

This repository contains a comprehensive implementation suite covering fundamental to advanced Natural Language Processing techniques. The four assignments demonstrate a strategic progression from classical machine learning approaches to state-of-the-art neural architectures, showcasing the evolution of NLP methodologies and their practical applications.

## ğŸ¯ Strategic Learning Progression

### Foundation â†’ Neural Networks â†’ Transformers â†’ Real-World Applications

```
A1: Classical ML     â†’  A2: Neural Networks  â†’  A3: Transformers    â†’  A4: LLM Analysis
â”œâ”€ Feature Engineering  â”œâ”€ Deep Learning       â”œâ”€ Self-Attention     â”œâ”€ Fact-Checking
â”œâ”€ Perceptron/Logistic  â”œâ”€ Word Embeddings     â”œâ”€ Language Modeling  â”œâ”€ Error Analysis
â””â”€ Sentiment Analysis   â””â”€ Optimization        â””â”€ Sequence Modeling  â””â”€ Model Evaluation
```

## ğŸ“š Assignment Breakdown

### Assignment 1: Classical Sentiment Classification
**Foundation: Feature Engineering & Traditional ML**

- **Problem**: Binary sentiment classification on movie reviews (Rotten Tomatoes dataset)
- **Techniques Implemented**:
  - **Perceptron Classifier** with bag-of-words features
  - **Logistic Regression** with advanced feature engineering
  - **Feature Extraction**: Unigrams, bigrams, better features
- **Key Learning**: Understanding the ML pipeline, feature design impact, and baseline establishment
- **Results**: Achieved competitive performance through strategic feature engineering

### Assignment 2: Neural Networks & Word Embeddings
**Evolution: From Sparse Features to Dense Representations**

- **Problem**: Enhanced sentiment classification using neural approaches
- **Techniques Implemented**:
  - **Deep Averaging Network (DAN)** architecture
  - **GloVe Word Embeddings** (50D and 300D)
  - **SGD Optimization** with manual gradient implementation
  - **PyTorch Integration** for neural network training
- **Key Innovation**: Transition from sparse bag-of-words to dense semantic representations
- **Results**: Demonstrated superior performance of neural methods over classical approaches

### Assignment 3: Transformer Architecture
**Breakthrough: Self-Attention & Language Modeling**

#### Part 1: Custom Transformer Implementation
- **Problem**: Letter counting task (BEFORE/BEFOREAFTER variants)
- **Architecture**: Built Transformer encoder from scratch
  - **Self-attention mechanism** with Q, K, V matrices
  - **Positional encoding** for sequence awareness
  - **Residual connections** and feed-forward networks
- **Results**: 99-100% accuracy on BEFORE task, 97-100% on BEFOREAFTER task

#### Part 2: Character-Level Language Modeling
- **Problem**: Next-character prediction on text8 dataset (100M Wikipedia characters)
- **Architecture**: 4-layer Transformer with nn.TransformerEncoder
- **Hyperparameters**: d_model=320, 20 epochs, batch_size=64
- **Results**: Final perplexity ~5.0 (training), ~1.09 (validation)

### Assignment 4: LLM Output Analysis & Fact-Checking
**Application: Real-World AI Safety & Verification**

- **Problem**: Fact-checking ChatGPT-generated biographies against Wikipedia
- **Dataset**: FActScore dataset (221 human-annotated fact instances)
- **Methods Implemented**:
  - **Word Overlap Baseline**: Bag-of-words similarity (76.92% accuracy)
  - **Neural Entailment**: DeBERTa-v3-base-mnli-fever-anli (84.16% accuracy)
- **Advanced Analysis**:
  - **Error Categorization**: Semantic confusion, implicit information, temporal mismatch
  - **LLM Comparison**: Systematic evaluation of different model outputs
- **Impact**: Addresses critical AI safety concerns in factual accuracy

## ğŸ”¬ Technical Innovations & Contributions

### Core NLP Techniques Mastered

1. **Feature Engineering Excellence**
   - Sparse vector representations with efficient indexing
   - Advanced n-gram features and linguistic preprocessing
   - Strategic vocabulary management and stopword handling

2. **Neural Architecture Design**
   - Custom Transformer implementation with mathematical precision
   - Attention mechanism visualization and interpretation
   - Memory-efficient training with gradient clipping

3. **Advanced Model Training**
   - Adaptive learning rate scheduling (cosine annealing)
   - Regularization techniques (L2 decay, early stopping)
   - Cross-validation and hyperparameter optimization

4. **Real-World Application Development**
   - Robust fact-checking pipeline with error analysis
   - Scalable evaluation frameworks with comprehensive metrics
   - Production-ready code with proper memory management

## ğŸ“Š Performance Achievements

| Assignment | Task | Best Method | Performance |
|------------|------|-------------|-------------|
| A1 | Sentiment Classification | Logistic Regression + Better Features | ~85% accuracy |
| A2 | Neural Sentiment Analysis | DAN + 300D GloVe | ~87% accuracy |
| A3 | Letter Counting | Custom Transformer | 99-100% accuracy |
| A3 | Language Modeling | 4-layer Transformer | 5.0 perplexity |
| A4 | Fact-Checking | Neural Entailment | 84.16% accuracy |

## ğŸ›  Technical Stack & Dependencies

### Core Technologies
- **Python 3.8+** - Primary programming language
- **PyTorch** - Deep learning framework
- **Transformers (Hugging Face)** - Pre-trained model integration
- **spaCy** - Advanced NLP preprocessing
- **NumPy/SciPy** - Numerical computations

### Specialized Libraries
- **NLTK** - Traditional NLP utilities
- **Matplotlib** - Visualization and analysis
- **tqdm** - Progress tracking
- **JSON/JSONL** - Data serialization

## ğŸ“ Educational Value & Learning Outcomes

### Fundamental Concepts Mastered
1. **Mathematical Foundations**: Gradient computation, optimization theory, attention mechanisms
2. **Software Engineering**: Modular design, efficient data structures, scalable architectures
3. **Research Methodology**: Systematic evaluation, error analysis, comparative studies
4. **Industry Applications**: Model deployment considerations, computational efficiency, real-world constraints

### Progressive Skill Development
- **Assignment 1**: Established ML fundamentals and feature engineering intuition
- **Assignment 2**: Transitioned to neural approaches with embedding representations
- **Assignment 3**: Mastered state-of-the-art architectures with custom implementations
- **Assignment 4**: Applied advanced techniques to critical real-world problems

## ğŸš€ Repository Structure

```
NLP-Applications/
â”œâ”€â”€ A1/                     # Classical ML & Feature Engineering
â”‚   â”œâ”€â”€ models.py          # Perceptron & Logistic Regression
â”‚   â”œâ”€â”€ sentiment_*.py     # Data handling & classification
â”‚   â””â”€â”€ logs/              # Training results & analysis
â”œâ”€â”€ A2/                     # Neural Networks & Embeddings
â”‚   â”œâ”€â”€ models.py          # Deep Averaging Network
â”‚   â”œâ”€â”€ optimization.py    # SGD implementation
â”‚   â””â”€â”€ data/              # GloVe embeddings
â”œâ”€â”€ A3/                     # Transformer Architecture
â”‚   â”œâ”€â”€ transformer.py     # Custom Transformer implementation
â”‚   â”œâ”€â”€ transformer_lm.py  # Language model architecture
â”‚   â””â”€â”€ logs/              # Training curves & results
â””â”€â”€ A4/                     # LLM Analysis & Fact-Checking
    â”œâ”€â”€ factcheck.py       # Fact-checking implementations
    â”œâ”€â”€ parts3and4.md     # Advanced error analysis
    â””â”€â”€ data/              # FActScore dataset
```

## ğŸ” Key Insights & Research Contributions

### Methodological Discoveries
1. **Feature Engineering Impact**: Demonstrated 10-15% accuracy gains through strategic feature design
2. **Embedding Dimensionality**: Showed optimal trade-offs between 50D and 300D representations
3. **Attention Visualization**: Provided interpretable insights into model decision-making
4. **Error Pattern Analysis**: Identified systematic failure modes in fact-checking systems

### Practical Applications
- **Industry-Ready Code**: All implementations follow production standards with proper error handling
- **Scalable Architectures**: Designed for extension to larger datasets and more complex tasks
- **Comprehensive Evaluation**: Includes statistical significance testing and cross-validation
- **Documentation Excellence**: Detailed README files with usage examples and performance analysis

## ğŸ¯ Future Extensions & Research Directions

1. **Multi-Modal Integration**: Extending fact-checking to include image and video evidence
2. **Cross-Lingual Applications**: Adapting architectures for multilingual NLP tasks
3. **Real-Time Systems**: Optimizing models for low-latency production deployment
4. **Explainable AI**: Enhancing interpretability for critical decision-making applications

## ğŸ“ˆ Impact & Significance

This portfolio demonstrates mastery of the complete NLP pipeline from theoretical foundations to practical applications. The progression from classical machine learning to cutting-edge transformer architectures mirrors the field's evolution and prepares for advanced research in areas like large language models, multimodal AI, and AI safety.

**Total Lines of Code**: ~3,000+ lines of production-quality Python
**Models Implemented**: 8+ different architectures from scratch
**Datasets Processed**: 4 different domains (sentiment, text generation, fact-checking)
**Performance Metrics**: Comprehensive evaluation across accuracy, perplexity, F1-score, and error analysis

---

*This repository represents a comprehensive journey through modern NLP, combining theoretical rigor with practical implementation skills essential for advanced AI research and development.*
# Assignment 4: Fact-checking Outputs from ChatGPT

## Problem Statement

This assignment focuses on analyzing and fact-checking outputs from large language models (LLMs) like ChatGPT. The primary goal is to understand what types of non-factual outputs LLMs can generate and develop methods to verify those outputs against Wikipedia. The task involves implementing fact-checking models and conducting error analysis of their mistakes.

## Dataset

The dataset is based on **FActScore** (Min et al., 2023), which investigates detecting errors in biographies generated by ChatGPT. The dataset consists of:

- **Facts**: Simple natural language sentences extracted from ChatGPT outputs
- **Passages**: Wikipedia passages retrieved using BM25 (sparse retrieval)
- **Labels**: Human annotations with "S" (supported), "NS" (not supported), or "IR" (irrelevant)

### Data Files
- `data/dev_labeled_ChatGPT.jsonl`: Human-annotated facts with labels
- `data/passages_bm25_ChatGPT_humfacts.jsonl`: Retrieved Wikipedia passages
- Total examples: **221 fact-checking instances**

## Architecture & Implementation

### Core Components

1. **FactExample**: Stores facts, passages, and labels
2. **EntailmentModel**: Wrapper for transformer-based entailment models
3. **FactChecker Classes**: Different approaches to fact-checking

### Implemented Models

#### Part 1: Word Overlap Method (`WordRecallThresholdFactChecker`)
- **Approach**: Bag-of-words overlap between facts and passages
- **Features**:
  - Text preprocessing (lowercasing, punctuation removal)
  - Sentence splitting using spaCy
  - Word overlap calculation with threshold-based classification
  - Stopword filtering and stemming

#### Part 2: Entailment Method (`EntailmentFactChecker`)
- **Model**: DeBERTa-v3-base-mnli-fever-anli (MoritzLaurer)
- **Approach**: Neural entailment classification
- **Features**:
  - Transformer-based sequence classification
  - Premise-hypothesis entailment scoring
  - Memory management with garbage collection
  - Sentence-level entailment checking

### Baseline Models
- **RandomGuessFactChecker**: Random S/NS prediction
- **AlwaysEntailedFactChecker**: Always predicts "S"

## Results

### Part 1: Word Overlap Method
```
Accuracy: 170/221 = 76.92%
Precision for S: 0.7713
Recall for S: 0.7713
F1 for S: 0.7713
Precision for NS: 0.7671
Recall for NS: 0.7671
F1 for NS: 0.7671
```

### Part 2: Entailment Method
```
Accuracy: 186/221 = 84.16%
Precision for S: 0.8969
Recall for S: 0.7768
F1 for S: 0.8325
Precision for NS: 0.7984
Recall for NS: 0.9083
F1 for NS: 0.8498
```

## Technical Analysis

### Performance Comparison
- **Entailment method** significantly outperforms word overlap (84.16% vs 76.92% accuracy)
- **Neural approach** shows better precision for supported facts (89.69% vs 77.13%)
- **Word overlap** method provides balanced performance across both classes
- **Processing time**: Entailment method takes ~3.34s per example due to transformer inference

### Key Insights
1. **Semantic Understanding**: Entailment models capture semantic relationships better than simple word overlap
2. **Precision-Recall Trade-off**: Entailment model shows higher precision for "S" but lower recall
3. **Computational Cost**: Neural methods require significantly more computational resources
4. **Robustness**: Word overlap provides a strong baseline with simpler implementation

## Error Analysis (Parts 3 & 4)

### Error Categories Identified

1. **Semantic Confusion**: Model fails to distinguish related concepts (e.g., engraver vs. printmaker)
2. **Implicit Information**: Incorrect inference of unstated information
3. **Temporal Mismatch**: Failure to match temporal information correctly
4. **Reference Resolution Failure**: Issues with resolving references across passage parts

### Error Statistics
| Error Type | Category | Count |
|------------|----------|-------|
| False Positive | Semantic Confusion | 4 |
| False Positive | Implicit Information | 6 |
| False Negative | Temporal Mismatch | 5 |
| False Negative | Reference Resolution Failure | 5 |

### LLM Output Analysis
Comparative analysis of different language models revealed:
- **Comprehensiveness** varies significantly between models
- **Practical applicability** differs in timeline realism and detail depth
- **Technical accuracy** issues in privacy considerations and implementation details

## File Structure

```
A4/
├── README.md                           # This file
├── factcheck.py                        # Core fact-checking implementations
├── factchecking_main.py                # Main execution script
├── error_analyzer.py                   # Error analysis utilities
├── generate_errors.py                  # Error generation tools
├── parts3and4.md                       # Detailed analysis for Parts 3 & 4
├── requirements.txt                     # Python dependencies
├── data/
│   ├── dev_labeled_ChatGPT.jsonl      # Labeled fact examples
│   ├── passages_bm25_ChatGPT_humfacts.jsonl  # Retrieved passages
│   └── FACTSCORE-LICENSE.txt          # Dataset license
└── logs/
    ├── part1.txt                       # Word overlap results
    └── part2.txt                       # Entailment method results
```

## Usage

### Running Different Methods
```bash
# Install dependencies
pip install -r requirements.txt

# Run word overlap method (Part 1)
python factchecking_main.py --mode word_overlap

# Run entailment method (Part 2)
python factchecking_main.py --mode entailment

# Run baseline methods
python factchecking_main.py --mode random
python factchecking_main.py --mode always_entail
```

### Dependencies
- Python 3.8+
- PyTorch
- Transformers (Hugging Face)
- spaCy
- tqdm
- Additional dependencies in `requirements.txt`

## Conclusion

This assignment demonstrates the challenges and approaches in automated fact-checking of LLM outputs. The entailment-based approach shows superior performance but at higher computational cost, while word overlap provides a practical baseline. The error analysis reveals specific failure modes that could guide future improvements in fact-checking systems.

The work highlights the importance of:
1. **Semantic understanding** over simple lexical matching
2. **Robust evaluation** across different error types
3. **Computational efficiency** considerations in practical applications
4. **Error analysis** for system improvement and understanding limitations